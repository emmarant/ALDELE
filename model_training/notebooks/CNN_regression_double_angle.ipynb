{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-setup\" data-toc-modified-id=\"Imports-and-setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and setup</a></span></li><li><span><a href=\"#Dataframes\" data-toc-modified-id=\"Dataframes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataframes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-pandas-dataframes-with-attributes:-filepath,-gal,-rc,-gau\" data-toc-modified-id=\"Create-pandas-dataframes-with-attributes:-filepath,-gal,-rc,-gau-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Create pandas dataframes with attributes: filepath, gal, rc, gau</a></span><ul class=\"toc-item\"><li><span><a href=\"#simulated-dataset\" data-toc-modified-id=\"simulated-dataset-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>simulated dataset</a></span></li></ul></li><li><span><a href=\"#split-dataframe-to-two:-one-will-serve-as-test-dataset\" data-toc-modified-id=\"split-dataframe-to-two:-one-will-serve-as-test-dataset-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>split dataframe to two: one will serve as test dataset</a></span></li><li><span><a href=\"#inspect-dataframes\" data-toc-modified-id=\"inspect-dataframes-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>inspect dataframes</a></span></li></ul></li><li><span><a href=\"#Set-parameters-dictionary\" data-toc-modified-id=\"Set-parameters-dictionary-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Set parameters dictionary</a></span></li><li><span><a href=\"#DataGenerators:-use-flow_from_dataframe-class\" data-toc-modified-id=\"DataGenerators:-use-flow_from_dataframe-class-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>DataGenerators: use flow_from_dataframe class</a></span></li><li><span><a href=\"#CNN-Model\" data-toc-modified-id=\"CNN-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CNN Model</a></span></li><li><span><a href=\"#Define-callbacks\" data-toc-modified-id=\"Define-callbacks-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Define callbacks</a></span></li><li><span><a href=\"#Model-fit\" data-toc-modified-id=\"Model-fit-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model fit</a></span></li><li><span><a href=\"#Plot-learning-curves\" data-toc-modified-id=\"Plot-learning-curves-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Plot learning curves</a></span></li><li><span><a href=\"#Evaluation-on-test-dataset\" data-toc-modified-id=\"Evaluation-on-test-dataset-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Evaluation on test dataset</a></span></li><li><span><a href=\"#Predict-on-single-images\" data-toc-modified-id=\"Predict-on-single-images-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Predict on single images</a></span></li><li><span><a href=\"#General-sandbox-area\" data-toc-modified-id=\"General-sandbox-area-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>General sandbox area</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DmkD091WVOo"
   },
   "source": [
    "\n",
    "CNN with regression and double output: rc and gal rotation (BOA beamline naming convention), or else y-axis and x-axis rotation, respectively (McStas naming convention).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9vKZSWZJBGm"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "from time import time\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import models, layers,optimizers, metrics, backend, losses\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Conv2D, MaxPool2D, GlobalMaxPool2D, Dropout,Flatten\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXBP5HzvISbB"
   },
   "source": [
    "Get the rotation values from the name of the files and create pandas dataframes with file paths and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pandas dataframes with attributes: filepath, gal, rc, gau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjCYssp6JBGs"
   },
   "outputs": [],
   "source": [
    "def get_rot_attributes(filepath):\n",
    "    \"\"\" Reads from name of image file the values of the three angles and unique file's ID\n",
    "        Keyword Arguments: \n",
    "        filepath -- the full path of the image file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path,file=os.path.split(filepath)\n",
    "        file,ext=os.path.splitext(file)\n",
    "        _,params = file.split('ellipse_rot_')\n",
    "        gal,rc,gau,ID_t6 = params.split('_')\n",
    "\n",
    "        return gal,rc,ID_t6\n",
    "    except Exception as e:\n",
    "        return None,None,None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path of directory where image files should be read from and create list of attributes from all files.\n",
    "\n",
    "data_dir_sim='full_path_location_of_images_directory' # choose directory where all images to be used are stored\n",
    "filepaths_sim=glob.glob(os.path.join(data_dir_sim,'*.jpg')) # if not jpg format, change extension to match. Make sure TF supports fileformat.\n",
    "random.shuffle(filepaths_sim)\n",
    "attributes_sim=list(map(get_rot_attributes,filepaths_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one dataframe with above attributes\n",
    "\n",
    "dfsim=pd.DataFrame(attributes_sim)\n",
    "dfsim['file']=filepaths_sim\n",
    "dfsim.columns=['gal','rc','ID_t6','file'] # columns of dataframe are: x-axis rotation value, y-axis rotation value, #of detector after lens, ID# of file\n",
    "\n",
    "dfsim['rc']=pd.to_numeric(dfsim['rc'],downcast=\"float\") \n",
    "dfsim['gal']=pd.to_numeric(dfsim['gal'],downcast=\"float\")\n",
    "dfsim['ID_t6']=pd.to_numeric(dfsim['ID_t6'],downcast=\"integer\")\n",
    "dfsim['file'] = dfsim['file'].astype('str') \n",
    "\n",
    "dfsim.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and check the dataframe\n",
    "\n",
    "dfsim = dfsim.sample(frac=1).reset_index(drop=True)\n",
    "dfsim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split dataframe to two: one will serve as test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataframe to a train (75% of initial df length), validation (15% of inital train length), and test (10% of initial df length) dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "tr=int(len(dfsim.index)*0.75)\n",
    "v=int(len(dfsim.index)*0.9)\n",
    "print(tr,v)\n",
    "\n",
    "dftrain=dfsim.iloc[:tr,:]\n",
    "dfvalid=dfsim.iloc[tr:v,:]\n",
    "dftest=dfsim.iloc[v:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inspect dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train: ',len(dftrain), '   valid: ',len(dfvalid),'    test dataframe:',len(dftest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfsim['rc'],bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit the following dictionary accordingly. Its entries - some of which are hyperparameters of the model - are used further down and are also saved in Neptune.ai**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gp5oQf0_MW09"
   },
   "outputs": [],
   "source": [
    "PARAMS = {'lr': 5e-5,   \n",
    "          'dropout': 0.0,\n",
    "          'batch_size': 32,\n",
    "          'n_epochs': 200,\n",
    "          'optimizer': 'ADAM',\n",
    "          'loss': 'MAE',\n",
    "          'metrics': 'RMSE, MAE',\n",
    "          'activations': 'relu, linear',\n",
    "          'notebook':'the name of the notebook used',\n",
    "          'image_input_shape' : (256,256,1),\n",
    "          'data_description' : ' ',\n",
    "          'dataset size' : '  ',\n",
    "          'testset predictions csv' :'testset_name.csv' ,\n",
    "          'run_name' : 'neptune run ID',\n",
    "          'save_model' : 'name_of_model_to_save.h5',\n",
    "          'run env' : ' ',   \n",
    "           'NOTES': ' General notes'      \n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataGenerators: use flow_from_dataframe class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create datasets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = PARAMS['batch_size']\n",
    "\n",
    "train_datagen = ImageDataGenerator(samplewise_center=True,samplewise_std_normalization=True)\n",
    "train = train_datagen.flow_from_dataframe(dftrain,x_col='file',y_col=['gal','rc'],class_mode='multi_output',batch_size=batch,target_size=(256,256),color_mode='grayscale')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(samplewise_center=True,samplewise_std_normalization=True)\n",
    "validation = valid_datagen.flow_from_dataframe(dfvalid,x_col='file',y_col=['gal','rc'],class_mode='multi_output',batch_size=batch,target_size=(256,256),color_mode='grayscale')\n",
    "\n",
    "# notice that for the testset generator shuffle is set to False. This way it's possible to copy the real values from the dataset in order to compare with final predictions on the dataset\n",
    "test_datagen = ImageDataGenerator(samplewise_center=True,samplewise_std_normalization=True)\n",
    "test = test_datagen.flow_from_dataframe(dftest,x_col='file',y_col=['gal','rc'],class_mode='multi_output',batch_size=batch,target_size=(256,256),color_mode='grayscale',shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean session and model if needed\n",
    "\n",
    "backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras does not have RMSE as loss function. Needs to be defined (note: it has MSE though).\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "colab_type": "code",
    "id": "g4SfBihMW54q",
    "outputId": "03e792cf-721d-4921-8f05-c2a48ac95942",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The model architecture and a printed summary at the end\n",
    "\n",
    "input_layer=Input(shape=(256,256,1))\n",
    "\n",
    "NNlayer=Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "NNlayer=Conv2D(64, (3, 3), activation='relu')(NNlayer)\n",
    "#NNlayer=BatchNormalization()(NNlayer)      \n",
    "NNlayer=MaxPool2D((2, 2))(NNlayer)\n",
    "NNlayer=Conv2D(128, (3, 3), activation='relu')(NNlayer)\n",
    "#NNlayer=BatchNormalization()(NNlayer)\n",
    "NNlayer=MaxPool2D((2, 2))(NNlayer)\n",
    "NNlayer=Conv2D(256, (3, 3), activation='relu')(NNlayer)\n",
    "#NNlayer=BatchNormalization()(NNlayer)\n",
    "NNlayer=MaxPool2D((2, 2))(NNlayer)\n",
    "NNlayer=Conv2D(256, (3, 3), activation='relu')(NNlayer)\n",
    "#NNlayer=BatchNormalization()(NNlayer)\n",
    "NNlayer=MaxPool2D((2, 2))(NNlayer)\n",
    "NNlayer=Flatten()(NNlayer)\n",
    "NNlayer=Dense(units=128,activation='relu')(NNlayer)\n",
    "NNlayer=Dropout(PARAMS['dropout'])(NNlayer)\n",
    "NNlayer= Dense(units=64, activation='relu')(NNlayer)\n",
    "NNlayer= Dense(units=16, activation='relu')(NNlayer)\n",
    "\n",
    "# Branching out begins here\n",
    "gal = Dense(units=1,activation='linear',name='gal')(NNlayer)\n",
    "rc = Dense(units=1,activation='linear',name='rc')(NNlayer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[gal,rc])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=PARAMS[\"lr\"]), \n",
    "              loss={'gal':'mae','rc':'mae'},\n",
    "              metrics={\"gal\":[metrics.RootMeanSquaredError()],\"rc\":[metrics.RootMeanSquaredError()]})  \n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPaJO-9AZ1mu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a graph of the model layers\n",
    "\n",
    "plot_model(model, to_file='model.png',show_shapes=True,rankdir='TB',show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----**invoke Neptune and define callbacks**-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your neptube token shoud go here, within the quotes Ommit if you do not wish to use neptune\n",
    "\n",
    "%env NEPTUNE_API_TOKEN=\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init(project='name-your-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run['Parameters']= PARAMS\n",
    "run['Name'] = PARAMS['run_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "neptune_monitor = NeptuneCallback(run=run, base_namespace='metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smyvx-ZHrj9p"
   },
   "outputs": [],
   "source": [
    "class TimingCallback(Callback):\n",
    "  def __init__(self):\n",
    "    self.logs=[]\n",
    "  def on_epoch_begin(self,epoch, logs={}):\n",
    "    self.starttime=time()\n",
    "  def on_epoch_end(self,epoch, logs={}):\n",
    "    self.logs.append(time()-self.starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQMW3KLbr7Ou"
   },
   "outputs": [],
   "source": [
    "timing_callback = TimingCallback()\n",
    "checkpoint=ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
    "callbacks = [neptune_monitor,checkpoint,timing_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----**train the model**-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "epoch_num=PARAMS[\"n_epochs\"]\n",
    "\n",
    "spe=len(train)\n",
    "val_steps=len(validation)\n",
    "\n",
    "\n",
    "history = model.fit(train,\n",
    "                    steps_per_epoch=spe,\n",
    "                    epochs=epoch_num,\n",
    "                    validation_data=validation,\n",
    "                    validation_steps=val_steps,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "model.save(PARAMS['save_model'])\n",
    "  \n",
    "run['model/'+PARAMS['save_model']].upload(PARAMS['save_model'])\n",
    "run['notebook/'+PARAMS['notebook']].upload(PARAMS['notebook'])                                \n",
    "run['model/model.png'].upload('model.png')                                \n",
    "\n",
    "metrics = model.metrics_names\n",
    "\n",
    "test_score = model.evaluate(test)\n",
    "pred = model.predict(test)\n",
    "\n",
    "\n",
    "\n",
    "run['test/'+metrics[0]].log(test_score[0])\n",
    "run['test/'+metrics[1]].log(test_score[1])\n",
    "run['test/'+metrics[2]].log(test_score[2]) \n",
    "\n",
    "                           \n",
    "\n",
    "\n",
    "dfpred = dftest[['gal','rc']].copy()\n",
    "\n",
    "dfpred['pred_gal']=pred[0].flatten()\n",
    "dfpred['pred_rc']=pred[1].flatten()\n",
    "dfpred.reset_index(drop=True,inplace=True)\n",
    "\n",
    "dfpred.to_csv(PARAMS['testset predictions csv'], index=False)\n",
    "run['test/'+PARAMS['testset predictions csv']].upload(PARAMS['testset predictions csv'])\n",
    "\n",
    "\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "Fn8stCN7eRe4",
    "outputId": "089e16de-a145-421b-bd86-75734a382fe9"
   },
   "outputs": [],
   "source": [
    "def learning_curves(model_history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
    "    axes[0,0].plot(history.history['loss'], label='Total training loss')\n",
    "    axes[0,0].plot(history.history['val_loss'], label='Total validation loss')\n",
    "    axes[0,0].set_xlabel('Epochs')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    axes[0,1].plot(history.history['rc_loss'], label=' rc training loss')\n",
    "    axes[0,1].plot(history.history['val_rc_loss'], label='rc validation loss')\n",
    "    axes[0,1].set_xlabel('Epochs')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    axes[1,0].plot(history.history['gal_loss'], label='gal training loss')\n",
    "    axes[1,0].plot(history.history['val_gal_loss'], label='gal validation loss')\n",
    "    axes[1,0].set_xlabel('Epochs')\n",
    "    axes[1,0].legend()\n",
    "\n",
    "learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('your_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate on entirety of test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model.evaluate(test)\n",
    "print('total loss:  ', test_score[0])\n",
    "print('gal loss: ' , test_score[1])\n",
    "print('rc loss: ',test_score[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on single images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('your_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose a file from the test dataframe\n",
    "\n",
    "index = random.randrange(len(dftest))\n",
    "img = image.load_img(dftest.iloc[index].file, target_size=(256, 256),color_mode='grayscale')\n",
    "\n",
    "img_exp = image.img_to_array(img,dtype=None)\n",
    "\n",
    "mean=img_exp.mean()\n",
    "std=img_exp.std()\n",
    "img_exp = (img_exp-mean)/std\n",
    "img_exp = np.expand_dims(img_exp, axis=0)\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(img_exp[0],cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "prediction = model.predict(img_exp)\n",
    "print('predicted gal =  ',prediction[0][0][0], 'predicted rc =  ',prediction[1][0][0])\n",
    "print('real gal = ',dftest.iloc[index].gal,'real rc = ',dftest.iloc[index].rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General sandbox area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(test[0][0][63],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scatter plots of predicted and real values fpr individual batches of the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(test[batch_num][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"y1 MAE:%.4f\" % mean_absolute_error(pred[0],test[batch_num][1][0]))\n",
    "print(\"y2 MAE:%.4f\" % mean_absolute_error(pred[1],test[batch_num][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(80)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x_ax,test[batch_num][1][0],  s=10, label=\"gal-real\",c='k')\n",
    "plt.plot(x_ax,pred[0], label=\"gal-pred\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x_ax, test[batch_num][1][1],  s=10, label=\"rc-real\",c='r')\n",
    "plt.plot(x_ax, pred[1], label=\"rc-pred\",c='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_with_regression_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "381.1875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
