{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-setup\" data-toc-modified-id=\"Imports-and-setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and setup</a></span></li><li><span><a href=\"#Dataframes\" data-toc-modified-id=\"Dataframes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataframes</a></span><ul class=\"toc-item\"><li><span><a href=\"#create-pandas-dataframes-with-attributes:-filepath,-gal,-rc,-gau\" data-toc-modified-id=\"create-pandas-dataframes-with-attributes:-filepath,-gal,-rc,-gau-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>create pandas dataframes with attributes: filepath, gal, rc, gau</a></span></li><li><span><a href=\"#split-dataframe-in-three:-training,-validation,-and-test-datasets\" data-toc-modified-id=\"split-dataframe-in-three:-training,-validation,-and-test-datasets-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>split dataframe in three: training, validation, and test datasets</a></span></li><li><span><a href=\"#inspect-dataframes\" data-toc-modified-id=\"inspect-dataframes-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>inspect dataframes</a></span></li></ul></li><li><span><a href=\"#Set-parameters-dictionary\" data-toc-modified-id=\"Set-parameters-dictionary-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Set parameters dictionary</a></span></li><li><span><a href=\"#DataGenerators:-use-flow_from_dataframe-class\" data-toc-modified-id=\"DataGenerators:-use-flow_from_dataframe-class-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>DataGenerators: use flow_from_dataframe class</a></span></li><li><span><a href=\"#CNN-Model\" data-toc-modified-id=\"CNN-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CNN Model</a></span></li><li><span><a href=\"#Define-callbacks\" data-toc-modified-id=\"Define-callbacks-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Define callbacks</a></span></li><li><span><a href=\"#Model-fit\" data-toc-modified-id=\"Model-fit-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model fit</a></span></li><li><span><a href=\"#Plot-learning-curves\" data-toc-modified-id=\"Plot-learning-curves-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Plot learning curves</a></span></li><li><span><a href=\"#Evaluation-on-test-dataset\" data-toc-modified-id=\"Evaluation-on-test-dataset-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Evaluation on test dataset</a></span></li><li><span><a href=\"#Predict-on-single-images\" data-toc-modified-id=\"Predict-on-single-images-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Predict on single images</a></span></li><li><span><a href=\"#General-sandbox-area\" data-toc-modified-id=\"General-sandbox-area-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>General sandbox area</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DmkD091WVOo"
   },
   "source": [
    "\n",
    "\n",
    "CNN with regression and single output: **rc OR gal** (BOA beamline naming convention), or else **y-axis OR x-axis** rotation, respectively (McStas naming convention).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: check TF version and availability of GPU\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9vKZSWZJBGm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import cv2\n",
    "from time import time\n",
    "\n",
    "from tensorflow.keras import models, layers, optimizers, metrics, backend, losses\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Conv2D, MaxPool2D, GlobalMaxPool2D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXBP5HzvISbB"
   },
   "source": [
    "get the rotation values from the name of the files and create pandas df with file paths and attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create pandas dataframes with attributes: filepath, gal, rc, gau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjCYssp6JBGs"
   },
   "outputs": [],
   "source": [
    "def get_rot_attributes(filepath):\n",
    "    \"\"\" Reads from name of image file the values of the three angles and unique file's ID\n",
    "        Keyword Arguments: \n",
    "        filepath -- the full path of the image file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path, file = os.path.split(filepath)\n",
    "        file, ext = os.path.splitext(file)\n",
    "        _, params = file.split('ellipse_rot_')\n",
    "        gal, rc, gau, ID_t6 = params.split('_')\n",
    "\n",
    "        return gal, rc, ID_t6\n",
    "    except Exception as e:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path of directory where image files should be read from and create list of attributes from all files.\n",
    "\n",
    "data_dir_sim = 'full_path_location_of_images_directory'  # choose directory where all images to be used are stored\n",
    "filepaths_sim = glob.glob(\n",
    "    os.path.join(data_dir_sim, '*.jpg')\n",
    ")  # if not jpg format, change extension to match. Make sure TF supports fileformat.\n",
    "random.shuffle(filepaths_sim)\n",
    "attributes_sim = list(map(get_rot_attributes, filepaths_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one dataframe with above attributes\n",
    "\n",
    "dfsim = pd.DataFrame(attributes_sim)\n",
    "dfsim['file'] = filepaths_sim\n",
    "dfsim.columns = [\n",
    "    'gal', 'rc', 'ID_t6', 'file'\n",
    "]  # columns of dataframe are: x-axis rotation value, y-axis rotation value, #of detector after lens, ID# of file\n",
    "\n",
    "dfsim['rc'] = pd.to_numeric(dfsim['rc'], downcast=\"float\")\n",
    "dfsim['gal'] = pd.to_numeric(dfsim['gal'], downcast=\"float\")\n",
    "dfsim['ID_t6'] = pd.to_numeric(dfsim['ID_t6'], downcast=\"integer\")\n",
    "dfsim['file'] = dfsim['file'].astype('str')\n",
    "\n",
    "dfsim.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataframe in place and check content\n",
    "\n",
    "dfsim = dfsim.sample(frac=1).reset_index(drop=True)\n",
    "dfsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split dataframe in three: training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataframe to a train (75% of initial df length), validation (15% of inital train length), and test (10% of initial df length) dataframes.\n",
    "Or adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = int(len(dfsim.index) * 0.75)\n",
    "v = int(len(dfsim.index) * 0.9)\n",
    "print(tr, v)\n",
    "\n",
    "dftrain = dfsim.iloc[:tr, :]\n",
    "dfvalid = dfsim.iloc[tr:v, :]\n",
    "dftest = dfsim.iloc[v:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inspect dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train: ', len(dftrain), '   valid: ', len(dfvalid),\n",
    "      '    test dataframe:', len(dftest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "mkB287V7BC2f",
    "outputId": "b21ee763-4013-421d-ec96-d08acc601b6b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftrain.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit the following dictionary accordingly. Its entries - some of which are hyperparameters of the model - are used further down and are also saved in neptune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'lr': 5e-5,\n",
    "    'dropout': 0.0,\n",
    "    'batch_size': 32,\n",
    "    'n_epochs': 200,\n",
    "    'optimizer': 'ADAM',\n",
    "    'loss': 'MAE',\n",
    "    'metrics': 'RMSE, MAE',\n",
    "    'activations': 'relu, linear',\n",
    "    'notebook': 'the name of the notebook used',\n",
    "    'image_input_shape': (256, 256, 1),\n",
    "    'data_description': ' ',\n",
    "    'dataset size': '  ',\n",
    "    'testset predictions csv': 'testset_name.csv',\n",
    "    'run_name': 'neptune run ID',\n",
    "    'save_model': 'name_of_model_to_save.h5',\n",
    "    'run env': ' ',\n",
    "    'NOTES': ' General notes'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataGenerators: use flow_from_dataframe class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'gal'  # which column from the dataset to use\n",
    "batch = PARAMS['batch_size']\n",
    "\n",
    "train_datagen = ImageDataGenerator(samplewise_center=True,\n",
    "                                   samplewise_std_normalization=True)\n",
    "train = train_datagen.flow_from_dataframe(dftrain,\n",
    "                                          x_col='file',\n",
    "                                          y_col=[col],\n",
    "                                          class_mode='raw',\n",
    "                                          batch_size=batch,\n",
    "                                          target_size=(256, 256),\n",
    "                                          color_mode='grayscale')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(samplewise_center=True,\n",
    "                                   samplewise_std_normalization=True)\n",
    "validation = valid_datagen.flow_from_dataframe(dfvalid,\n",
    "                                               x_col='file',\n",
    "                                               y_col=[col],\n",
    "                                               class_mode='raw',\n",
    "                                               batch_size=batch,\n",
    "                                               target_size=(256, 256),\n",
    "                                               color_mode='grayscale')\n",
    "\n",
    "# For the testset generator 'shuffle' is set to 'False'. This way it's possible to copy the real values from the dataset in order to compare with final predictions on the dataset\n",
    "test_datagen = ImageDataGenerator(samplewise_center=True,\n",
    "                                  samplewise_std_normalization=True)\n",
    "test = test_datagen.flow_from_dataframe(dftest,\n",
    "                                        x_col='file',\n",
    "                                        y_col=[col],\n",
    "                                        class_mode='raw',\n",
    "                                        batch_size=batch,\n",
    "                                        target_size=(256, 256),\n",
    "                                        color_mode='grayscale',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean session and model if needed\n",
    "\n",
    "backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make sure to choose the right output angle in the model below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "colab_type": "code",
    "id": "g4SfBihMW54q",
    "outputId": "03e792cf-721d-4921-8f05-c2a48ac95942",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The model architecture and a printed summary at the end\n",
    "\n",
    "input_layer = Input(shape=(256, 256, 1))\n",
    "\n",
    "NNlayer = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "NNlayer = Conv2D(64, (3, 3), activation='relu')(NNlayer)\n",
    "#NNlayer=BatchNormalization()(NNlayer)\n",
    "NNlayer = MaxPool2D((2, 2))(NNlayer)\n",
    "NNlayer = Conv2D(128, (3, 3), activation='relu')(NNlayer)\n",
    "#NNlayer=BatchNormalization()(NNlayer)\n",
    "NNlayer = MaxPool2D((2, 2))(NNlayer)\n",
    "NNlayer = Conv2D(256, (3, 3), activation='relu')(NNlayer)\n",
    "#NNlayer=BatchNormalization()(NNlayer)\n",
    "NNlayer = MaxPool2D((2, 2))(NNlayer)\n",
    "NNlayer = Conv2D(256, (3, 3), activation='relu')(NNlayer)\n",
    "#NNlayer=BatchNormalization()(NNlayer)\n",
    "NNlayer = MaxPool2D((2, 2))(NNlayer)\n",
    "NNlayer = Flatten()(NNlayer)\n",
    "NNlayer = Dense(units=128, activation='relu')(NNlayer)\n",
    "NNlayer = Dropout(PARAMS['dropout'])(NNlayer)\n",
    "NNlayer = Dense(units=64, activation='relu')(NNlayer)\n",
    "NNlayer = Dense(units=16, activation='relu')(NNlayer)\n",
    "\n",
    "#### CHOOSE WHICH ANGLE TO HAVE AS OUTPUT####\n",
    "gal = Dense(units=1, activation='linear', name='gal')(NNlayer)\n",
    "#rc = Dense(units=1,activation='linear',name='rc')(NNlayer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[gal])\n",
    "\n",
    "#### EDIT TO CHOOSE CORRECT ANGLE: 'gal' or 'rc'\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=PARAMS[\"lr\"]),\n",
    "              loss={'gal': 'mae'},\n",
    "              metrics={\"gal\": [metrics.RootMeanSquaredError()]})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPaJO-9AZ1mu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a graph of the model layers\n",
    "\n",
    "plot_model(model,\n",
    "           to_file='model.png',\n",
    "           show_shapes=True,\n",
    "           rankdir='TB',\n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----**invoke neptune and define callbacks**-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NEPTUNE_API_TOKEN=\"your neptune token should go here, if using neptune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init(project='your-projects-neptune-name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "run['Parameters'] = PARAMS\n",
    "run['Name'] = PARAMS['run_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "neptune_monitor = NeptuneCallback(run=run, base_namespace='metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smyvx-ZHrj9p"
   },
   "outputs": [],
   "source": [
    "class TimingCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(time() - self.starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQMW3KLbr7Ou"
   },
   "outputs": [],
   "source": [
    "timing_callback = TimingCallback()\n",
    "checkpoint = ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
    "callbacks = [neptune_monitor, checkpoint, timing_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----**train the model**-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_num = PARAMS[\"n_epochs\"]\n",
    "\n",
    "spe = len(train)\n",
    "val_steps = len(validation)\n",
    "\n",
    "history = model.fit(train,\n",
    "                    steps_per_epoch=spe,\n",
    "                    epochs=epoch_num,\n",
    "                    validation_data=validation,\n",
    "                    validation_steps=val_steps,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "model.save(PARAMS['save_model'])\n",
    "\n",
    "run['model/' + PARAMS['save_model']].upload(PARAMS['save_model'])\n",
    "run['notebook/' + PARAMS['notebook']].upload(PARAMS['notebook'])\n",
    "run['model/model.png'].upload('model.png')\n",
    "\n",
    "metrics = model.metrics_names\n",
    "test_score = model.evaluate(test)\n",
    "pred = model.predict(test)\n",
    "\n",
    "run['test/' + metrics[0]].log(test_score[0])\n",
    "\n",
    "dfpred = dftest[['gal']].copy()\n",
    "\n",
    "dfpred['pred_gal'] = pred.flatten()\n",
    "dfpred.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dfpred.to_csv(PARAMS['testset predictions csv'], index=False)\n",
    "run['test/' + PARAMS['testset predictions csv']].upload(\n",
    "    PARAMS['testset predictions csv'])\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "Fn8stCN7eRe4",
    "outputId": "089e16de-a145-421b-bd86-75734a382fe9"
   },
   "outputs": [],
   "source": [
    "def learning_curves(model_history):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    axes[0, 0].plot(history.history['loss'], label='Total training loss')\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Total validation loss')\n",
    "    axes[0, 0].set_xlabel('Epochs')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "\n",
    "learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_name.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate on entirety of test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model.evaluate(test)\n",
    "print('total loss:  ', test_score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on single images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_name.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose a file from the test dataframe\n",
    "\n",
    "index = random.randrange(len(dftest))\n",
    "img = image.load_img(dftest.iloc[index].file,\n",
    "                     target_size=(256, 256),\n",
    "                     color_mode='grayscale')\n",
    "\n",
    "img_exp = image.img_to_array(img, dtype=None)\n",
    "\n",
    "mean = img_exp.mean()\n",
    "std = img_exp.std()\n",
    "img_exp = (img_exp - mean) / std\n",
    "img_exp = np.expand_dims(img_exp, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_exp[0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "prediction = model.predict(img_exp)\n",
    "print('predicted angle =  ', prediction[0][0][0])\n",
    "print('real gal = ', dftest.iloc[index].gal, 'real rc = ',\n",
    "      dftest.iloc[index].rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General sandbox area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scatter plots of predicted and real values fpr individual batches of the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test[batch_num][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y1 MAE:%.4f\" % mean_absolute_error(pred[0], test[batch_num][1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(80)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(x_ax, test[batch_num][1][0], s=10, label=\"real\", c='k')\n",
    "plt.plot(x_ax, pred[0], label=\"pred\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_with_regression_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
